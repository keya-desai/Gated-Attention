{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Review Classifier using Soft Attention\n",
    "\n",
    "GA-Net code without Auxilliary Network  \n",
    "Backbone network = BiLSTM or LSTM  \n",
    "\n",
    "Normal softmax function is used instead of masked softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "D0T3nxESJstz",
    "outputId": "7341ce37-5134-467c-9d54-16650d84097e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip3 install torchtext==0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a9sHZi8rIXBC"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions\n",
    "import torch.optim as optim\n",
    "from torch import nn \n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "\n",
    "# from models.LSTM import LSTMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6CtSKjSRaQdd"
   },
   "outputs": [],
   "source": [
    "# model.py\n",
    "\n",
    "class GANet(torch.nn.Module):\n",
    "    def __init__(self, batch_size, num_classes, mlp_out_size, vocab_size, embedding_length, weights, aux_hidden_size = 100, backbone_hidden_size = 100, tau = 1, biDirectional_aux = False, biDirectional_backbone = False):\n",
    "        super(GANet, self).__init__() \n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        output_size : 6 = (For TREC dataset)\n",
    "        hidden_sie : Size of the hidden_state of the LSTM   (// Later BiLSTM)\n",
    "        vocab_size : Size of the vocabulary containing unique words\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
    "\n",
    "        --------\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_length = embedding_length\n",
    "        self.aux_hidden_size = aux_hidden_size\n",
    "        self.backbone_hidden_size = backbone_hidden_size \n",
    "        self.mlp_out_size = mlp_out_size\n",
    "        self.biDirectional_aux = biDirectional_aux\n",
    "        self.biDirectional_backbone = biDirectional_backbone\n",
    "        self.tau = tau\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "        self.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n",
    "\n",
    "        self.backbone = BackboneNet(self.batch_size, self.backbone_hidden_size, self.embedding_length, self.biDirectional_backbone)\n",
    "\n",
    "        if(self.biDirectional_backbone):\n",
    "            self.mlp = MLP(self.backbone_hidden_size * 2, self.mlp_out_size)\n",
    "            self.FF = nn.Linear(self.backbone_hidden_size * 2,num_classes)\n",
    "        else:\n",
    "            self.mlp = MLP(self.backbone_hidden_size, self.mlp_out_size)\n",
    "            self.FF = nn.Linear(self.backbone_hidden_size,num_classes)\n",
    "\n",
    "    \n",
    "    def forward(self,input_sequence, is_train = True):\n",
    "        input_ = self.word_embeddings(input_sequence)\n",
    "#         g_t, p_t = self.auxiliary(input_, is_train)\n",
    "        out_lstm = self.backbone(input_)\n",
    "        e_t = self.mlp(out_lstm)\n",
    "        alpha = torch.softmax(e_t, dim = 1)\n",
    "        c_t = torch.bmm(alpha.transpose(1,2), out_lstm)\n",
    "        logits = self.FF(c_t)\n",
    "        final_output = torch.softmax(logits, dim = -1)\n",
    "        # final_output = final_output.max(2)[1]\n",
    "        final_output = final_output.squeeze(1)\n",
    "\n",
    "        return final_output, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YT9xWP_MAVmq"
   },
   "outputs": [],
   "source": [
    "class BackboneNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
    "        backbone_hidden_size : Size of the hidden_state of the LSTM   (* Later BiLSTM, check dims for BiLSTM *)\n",
    "        embedding_length : Embeddding dimension of GloVe word embeddings\n",
    "        --------\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, backbone_hidden_size, embedding_length, biDirectional = False, num_layers = 2):\n",
    "\n",
    "        super(BackboneNet, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = backbone_hidden_size\n",
    "        self.embedding_length = embedding_length\n",
    "        self.biDirectional\t= biDirectional\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.backbone_lstm = nn.LSTM(self.embedding_length, self.hidden_size, bidirectional = self.biDirectional, batch_first = True, num_layers = self.num_layers)   # Dropout  \n",
    "\n",
    "    def forward(self, input_sequence, batch_size=None):\n",
    "        out_lstm, (final_hidden_state, final_cell_state) = self.backbone_lstm(input_sequence)   # ouput dim: ( batch_size x seq_len x hidden_size )\n",
    "        return out_lstm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_eU0HR-j8lW"
   },
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.ff_1 = nn.Linear(self.input_dim, self.output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ff_2 = nn.Linear(self.output_dim,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        out_1 = self.ff_1(x)\n",
    "        out_relu = self.relu(out_1)\n",
    "        out_2 = self.ff_2(out_relu)\n",
    "        out_sigmoid = self.sigmoid(out_2)\n",
    "\n",
    "        return out_sigmoid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Swz_WT2mS3zq"
   },
   "outputs": [],
   "source": [
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)\n",
    "    \n",
    "def train_model(model, optim, train_iter, epoch, batch_size):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    # model.cuda()\n",
    "#     optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    steps = 0\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "        text = batch.text[0]\n",
    "        target = batch.label\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        if torch.cuda.is_available():\n",
    "            text = text.cuda()\n",
    "            target = target.cuda()\n",
    "        if (text.size()[0] is not batch_size):# One of the batch returned by BucketIterator has length different than 32.\n",
    "            continue\n",
    "        optim.zero_grad()\n",
    "        prediction, alpha = model(text, is_train = True)\n",
    "        # print(\"prediction = \", prediction.shape)\n",
    "        # print(\"target = \", target.shape)\n",
    "        # print(\"prediction = \", prediction)\n",
    "        # print(\"target = \", target)\n",
    "\n",
    "        # Defualt - Cross entropy loss funtion\n",
    "        loss = loss_fn(prediction, target)\n",
    "        \n",
    "        # print(\"loss = \", loss)\n",
    "        \n",
    "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
    "        acc = 100.0 * num_corrects/len(batch)\n",
    "        loss.backward()\n",
    "        clip_gradient(model, 1e-1)\n",
    "        optim.step()\n",
    "        steps += 1\n",
    "        \n",
    "        # if steps % 10 == 0:\n",
    "            # print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n",
    "            # break\n",
    "        \n",
    "        total_epoch_loss += loss.item()\n",
    "        total_epoch_acc += acc.item()\n",
    "\n",
    "        \n",
    "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)\n",
    "\n",
    "def eval_model(model, val_iter):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    total_attention =  0\n",
    "    total_samples = 0 \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_iter):\n",
    "            text = batch.text[0]\n",
    "            if (text.size()[0] is not 32):\n",
    "                continue\n",
    "            target = batch.label\n",
    "            target = torch.autograd.Variable(target).long()\n",
    "            if torch.cuda.is_available():\n",
    "                text = text.cuda()\n",
    "                target = target.cuda()\n",
    "            prediction, alpha = model(text, is_train = False)\n",
    "            # Sanity check\n",
    "            # print(\"Test Prediction: \", prediction)\n",
    "            \n",
    "            # Defualt - Cross entropy loss funtion\n",
    "            loss =  loss_fn(prediction, target)\n",
    "            \n",
    "            if math.isnan(loss.item()):\n",
    "                print(prediction, target)\n",
    "            \n",
    "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            acc = 100.0 * num_corrects/len(batch)\n",
    "            total_epoch_loss += loss.item()\n",
    "            total_epoch_acc += acc.item()\n",
    "            \n",
    "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vAjlrmkGSEQQ"
   },
   "outputs": [],
   "source": [
    "# data.py\n",
    "def load_IMDB_data(batch_size= 32, embedding_length = 100):\n",
    "    # set up fields\n",
    "    tokenize = lambda x: x.split()\n",
    "    TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length=100)\n",
    "    # LABEL = data.LabelField()\n",
    "    LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "    # make splits for data\n",
    "    train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
    "    train, valid = train.split() \n",
    "    \n",
    "    # build the vocabulary\n",
    "    TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=embedding_length))\n",
    "    LABEL.build_vocab(train)\n",
    "    print(LABEL.vocab.__dict__)\n",
    "\n",
    "    # make iterator for splits\n",
    "    train_iter, valid_iter, test_iter = data.BucketIterator.splits(\n",
    "      (train, valid, test), batch_size= batch_size, device=0)\n",
    "\n",
    "    word_embeddings = TEXT.vocab.vectors\n",
    "    vocab_size = len(TEXT.vocab)\n",
    "\n",
    "    return TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "xHuvlEncW_tw",
    "outputId": "ea08c54e-f4c1-4785-cf86-c072a8dd620f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'freqs': Counter({'pos': 8779, 'neg': 8721}), 'itos': ['pos', 'neg'], 'unk_index': None, 'stoi': defaultdict(None, {'pos': 0, 'neg': 1}), 'vectors': None}\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter = load_IMDB_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PImJSOJmzQgT"
   },
   "outputs": [],
   "source": [
    "# Over-writing the loss function to simple cross entropy loss\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "learning_rate = 2e-5\n",
    "batch_size = 32\n",
    "output_size = 2\n",
    "hidden_size = 256\n",
    "embedding_length = 100\n",
    "num_classes = 2\n",
    "mlp_out_size = 32\n",
    "weights = word_embeddings\n",
    "aux_hidden_size = 100\n",
    "batch_hidden_size = 100\n",
    "tau = 1\n",
    "\n",
    "model = GANet(batch_size, num_classes, mlp_out_size, vocab_size, embedding_length, weights, biDirectional_backbone=True)\n",
    "optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"IMDB_Soft_Attn_100_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training_stats.to_csv(\"IMDB_Soft_Attn_100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print model's state_dict\n",
    "# print(\"Model's state_dict:\")\n",
    "# for param_tensor in model.state_dict():\n",
    "#     print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# # Print optimizer's state_dict\n",
    "# print(\"Optimizer's state_dict:\")\n",
    "# for var_name in optim.state_dict():\n",
    "#     print(var_name, \"\\t\", optim.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GANet(\n",
       "  (word_embeddings): Embedding(202771, 100)\n",
       "  (backbone): BackboneNet(\n",
       "    (backbone_lstm): LSTM(100, 100, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (ff_1): Linear(in_features=200, out_features=32, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (ff_2): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       "  (FF): Linear(in_features=200, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = GANet(batch_size, num_classes, mlp_out_size, vocab_size, embedding_length, weights, tau= tau, biDirectional_aux=True, biDirectional_backbone=True)\n",
    "loaded_model.load_state_dict(torch.load('IMDB_Soft_Attn_100_final'))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 830
    },
    "colab_type": "code",
    "id": "EcU6SSW8bDln",
    "outputId": "49069342-b9dd-4d4c-d2f3-6509dd836057",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "*** Least validation loss\n",
      "Train Loss: 0.603, Train Acc: 67.57%\n",
      "Val Loss: 0.543198, Val Acc: 74.75%\n",
      "-------------\n",
      "Epoch: 02\n",
      "*** Least validation loss\n",
      "Train Loss: 0.477, Train Acc: 82.64%\n",
      "Val Loss: 0.532478, Val Acc: 76.20%\n",
      "-------------\n",
      "Epoch: 03\n",
      "Train Loss: 0.414, Train Acc: 89.51%\n",
      "Val Loss: 0.547901, Val Acc: 75.12%\n",
      "-------------\n",
      "Epoch: 04\n",
      "*** Least validation loss\n",
      "Train Loss: 0.382, Train Acc: 92.82%\n",
      "Val Loss: 0.521762, Val Acc: 77.85%\n",
      "-------------\n",
      "Epoch: 05\n",
      "Train Loss: 0.364, Train Acc: 94.57%\n",
      "Val Loss: 0.532190, Val Acc: 76.72%\n",
      "-------------\n",
      "Epoch: 06\n",
      "*** Least validation loss\n",
      "Train Loss: 0.355, Train Acc: 95.58%\n",
      "Val Loss: 0.521062, Val Acc: 78.16%\n",
      "-------------\n",
      "Epoch: 07\n",
      "Train Loss: 0.351, Train Acc: 96.01%\n",
      "Val Loss: 0.534808, Val Acc: 76.74%\n",
      "-------------\n",
      "Epoch: 08\n",
      "Train Loss: 0.345, Train Acc: 96.54%\n",
      "Val Loss: 0.528122, Val Acc: 77.41%\n",
      "-------------\n",
      "Epoch: 09\n",
      "Train Loss: 0.344, Train Acc: 96.66%\n",
      "Val Loss: 0.554390, Val Acc: 74.89%\n",
      "-------------\n",
      "Epoch: 10\n",
      "Train Loss: 0.342, Train Acc: 96.84%\n",
      "Val Loss: 0.538099, Val Acc: 76.52%\n",
      "-------------\n",
      "Epoch: 11\n",
      "Train Loss: 0.343, Train Acc: 96.77%\n",
      "Val Loss: 0.525647, Val Acc: 77.59%\n",
      "-------------\n",
      "Epoch: 12\n",
      "*** Least validation loss\n",
      "Train Loss: 0.340, Train Acc: 97.07%\n",
      "Val Loss: 0.515896, Val Acc: 78.72%\n",
      "-------------\n",
      "Epoch: 13\n",
      "Train Loss: 0.340, Train Acc: 97.04%\n",
      "Val Loss: 0.521526, Val Acc: 78.02%\n",
      "-------------\n",
      "Epoch: 14\n",
      "Train Loss: 0.337, Train Acc: 97.37%\n",
      "Val Loss: 0.527198, Val Acc: 77.65%\n",
      "-------------\n",
      "Epoch: 15\n",
      "Train Loss: 0.337, Train Acc: 97.36%\n",
      "Val Loss: 0.521828, Val Acc: 77.99%\n",
      "-------------\n",
      "Epoch: 16\n",
      "Train Loss: 0.337, Train Acc: 97.38%\n",
      "Val Loss: 0.530417, Val Acc: 77.10%\n",
      "-------------\n",
      "Epoch: 17\n",
      "Train Loss: 0.335, Train Acc: 97.57%\n",
      "Val Loss: 0.527148, Val Acc: 77.59%\n",
      "-------------\n",
      "Epoch: 18\n",
      "Train Loss: 0.336, Train Acc: 97.43%\n",
      "Val Loss: 0.526707, Val Acc: 77.62%\n",
      "-------------\n",
      "Epoch: 19\n",
      "Train Loss: 0.335, Train Acc: 97.61%\n",
      "Val Loss: 0.523978, Val Acc: 77.97%\n",
      "-------------\n",
      "Epoch: 20\n",
      "Train Loss: 0.335, Train Acc: 97.61%\n",
      "Val Loss: 0.524181, Val Acc: 77.85%\n",
      "-------------\n",
      "Epoch: 21\n",
      "Train Loss: 0.333, Train Acc: 97.80%\n",
      "Val Loss: 0.523221, Val Acc: 78.10%\n",
      "-------------\n",
      "Epoch: 22\n",
      "Train Loss: 0.333, Train Acc: 97.83%\n",
      "Val Loss: 0.527908, Val Acc: 77.53%\n",
      "-------------\n",
      "Epoch: 23\n",
      "Train Loss: 0.334, Train Acc: 97.68%\n",
      "Val Loss: 0.518926, Val Acc: 78.42%\n",
      "-------------\n",
      "Epoch: 24\n",
      "Train Loss: 0.333, Train Acc: 97.79%\n",
      "Val Loss: 0.523701, Val Acc: 77.99%\n",
      "-------------\n",
      "Epoch: 25\n",
      "Train Loss: 0.332, Train Acc: 97.90%\n",
      "Val Loss: 0.535658, Val Acc: 76.84%\n",
      "-------------\n",
      "Epoch: 26\n",
      "Train Loss: 0.331, Train Acc: 97.99%\n",
      "Val Loss: 0.528777, Val Acc: 77.35%\n",
      "-------------\n",
      "Epoch: 27\n",
      "Train Loss: 0.331, Train Acc: 98.03%\n",
      "Val Loss: 0.530899, Val Acc: 77.35%\n",
      "-------------\n",
      "Epoch: 28\n",
      "Train Loss: 0.330, Train Acc: 98.07%\n",
      "Val Loss: 0.531781, Val Acc: 77.15%\n",
      "-------------\n",
      "Epoch: 29\n",
      "Train Loss: 0.330, Train Acc: 98.07%\n",
      "Val Loss: 0.533027, Val Acc: 76.99%\n",
      "-------------\n",
      "Epoch: 30\n",
      "Train Loss: 0.331, Train Acc: 98.03%\n",
      "Val Loss: 0.531907, Val Acc: 77.15%\n",
      "-------------\n",
      "Epoch: 31\n",
      "Train Loss: 0.330, Train Acc: 98.07%\n",
      "Val Loss: 0.529563, Val Acc: 77.18%\n",
      "-------------\n",
      "Epoch: 32\n",
      "Train Loss: 0.329, Train Acc: 98.13%\n",
      "Val Loss: 0.525057, Val Acc: 77.78%\n",
      "-------------\n",
      "Epoch: 33\n",
      "Train Loss: 0.330, Train Acc: 98.13%\n",
      "Val Loss: 0.531392, Val Acc: 77.17%\n",
      "-------------\n",
      "Epoch: 34\n",
      "Train Loss: 0.329, Train Acc: 98.23%\n",
      "Val Loss: 0.519658, Val Acc: 78.40%\n",
      "-------------\n",
      "Epoch: 35\n",
      "Train Loss: 0.328, Train Acc: 98.26%\n",
      "Val Loss: 0.523666, Val Acc: 77.95%\n",
      "-------------\n",
      "Epoch: 36\n",
      "Train Loss: 0.328, Train Acc: 98.29%\n",
      "Val Loss: 0.525407, Val Acc: 77.77%\n",
      "-------------\n",
      "Epoch: 37\n",
      "Train Loss: 0.328, Train Acc: 98.28%\n",
      "Val Loss: 0.523705, Val Acc: 77.97%\n",
      "-------------\n",
      "Epoch: 38\n",
      "Train Loss: 0.329, Train Acc: 98.13%\n",
      "Val Loss: 0.531661, Val Acc: 77.21%\n",
      "-------------\n",
      "Epoch: 39\n",
      "Train Loss: 0.329, Train Acc: 98.21%\n",
      "Val Loss: 0.522654, Val Acc: 78.02%\n",
      "-------------\n",
      "Epoch: 40\n",
      "Train Loss: 0.328, Train Acc: 98.26%\n",
      "Val Loss: 0.518054, Val Acc: 78.60%\n",
      "-------------\n",
      "Epoch: 41\n",
      "Train Loss: 0.328, Train Acc: 98.31%\n",
      "Val Loss: 0.537175, Val Acc: 76.45%\n",
      "-------------\n",
      "Epoch: 42\n",
      "Train Loss: 0.328, Train Acc: 98.32%\n",
      "Val Loss: 0.526320, Val Acc: 77.73%\n",
      "-------------\n",
      "Epoch: 43\n",
      "Train Loss: 0.328, Train Acc: 98.32%\n",
      "Val Loss: 0.528077, Val Acc: 77.71%\n",
      "-------------\n",
      "Epoch: 44\n",
      "Train Loss: 0.328, Train Acc: 98.31%\n",
      "Val Loss: 0.521775, Val Acc: 78.14%\n",
      "-------------\n",
      "Epoch: 45\n",
      "Train Loss: 0.328, Train Acc: 98.30%\n",
      "Val Loss: 0.524096, Val Acc: 77.90%\n",
      "-------------\n",
      "Epoch: 46\n",
      "Train Loss: 0.327, Train Acc: 98.38%\n",
      "Val Loss: 0.521760, Val Acc: 78.19%\n",
      "-------------\n",
      "Epoch: 47\n",
      "Train Loss: 0.327, Train Acc: 98.40%\n",
      "Val Loss: 0.525464, Val Acc: 77.89%\n",
      "-------------\n",
      "Epoch: 48\n",
      "Train Loss: 0.327, Train Acc: 98.40%\n",
      "Val Loss: 0.520558, Val Acc: 78.23%\n",
      "-------------\n",
      "Epoch: 49\n",
      "Train Loss: 0.326, Train Acc: 98.45%\n",
      "Val Loss: 0.526782, Val Acc: 77.66%\n",
      "-------------\n",
      "Epoch: 50\n",
      "Train Loss: 0.326, Train Acc: 98.47%\n",
      "Val Loss: 0.527066, Val Acc: 77.54%\n",
      "-------------\n",
      "Epoch: 51\n",
      "Train Loss: 0.326, Train Acc: 98.48%\n",
      "Val Loss: 0.527752, Val Acc: 77.57%\n",
      "-------------\n",
      "Epoch: 52\n",
      "Train Loss: 0.326, Train Acc: 98.50%\n",
      "Val Loss: 0.525844, Val Acc: 77.77%\n",
      "-------------\n",
      "Epoch: 53\n",
      "Train Loss: 0.326, Train Acc: 98.50%\n",
      "Val Loss: 0.524245, Val Acc: 77.86%\n",
      "-------------\n",
      "Epoch: 54\n",
      "Train Loss: 0.326, Train Acc: 98.50%\n",
      "Val Loss: 0.526222, Val Acc: 77.63%\n",
      "-------------\n",
      "Epoch: 55\n",
      "Train Loss: 0.327, Train Acc: 98.43%\n",
      "Val Loss: 0.523698, Val Acc: 78.01%\n",
      "-------------\n",
      "Epoch: 56\n",
      "Train Loss: 0.326, Train Acc: 98.50%\n",
      "Val Loss: 0.523974, Val Acc: 77.74%\n",
      "-------------\n",
      "Epoch: 57\n",
      "Train Loss: 0.325, Train Acc: 98.55%\n",
      "Val Loss: 0.522349, Val Acc: 78.11%\n",
      "-------------\n",
      "Epoch: 58\n",
      "Train Loss: 0.325, Train Acc: 98.57%\n",
      "Val Loss: 0.521471, Val Acc: 78.22%\n",
      "-------------\n",
      "Epoch: 59\n",
      "Train Loss: 0.325, Train Acc: 98.59%\n",
      "Val Loss: 0.521803, Val Acc: 78.15%\n",
      "-------------\n",
      "Epoch: 60\n",
      "Train Loss: 0.325, Train Acc: 98.59%\n",
      "Val Loss: 0.521637, Val Acc: 78.15%\n",
      "-------------\n",
      "Epoch: 61\n",
      "Train Loss: 0.325, Train Acc: 98.59%\n",
      "Val Loss: 0.522956, Val Acc: 78.14%\n",
      "-------------\n",
      "Epoch: 62\n",
      "Train Loss: 0.325, Train Acc: 98.59%\n",
      "Val Loss: 0.525138, Val Acc: 77.89%\n",
      "-------------\n",
      "Epoch: 63\n",
      "Train Loss: 0.325, Train Acc: 98.61%\n",
      "Val Loss: 0.524487, Val Acc: 77.90%\n",
      "-------------\n",
      "Epoch: 64\n",
      "Train Loss: 0.325, Train Acc: 98.62%\n",
      "Val Loss: 0.523400, Val Acc: 78.14%\n",
      "-------------\n",
      "Epoch: 65\n",
      "Train Loss: 0.325, Train Acc: 98.61%\n",
      "Val Loss: 0.524113, Val Acc: 77.97%\n",
      "-------------\n",
      "Epoch: 66\n",
      "Train Loss: 0.325, Train Acc: 98.62%\n",
      "Val Loss: 0.524091, Val Acc: 78.01%\n",
      "-------------\n",
      "Epoch: 67\n",
      "Train Loss: 0.325, Train Acc: 98.62%\n",
      "Val Loss: 0.524083, Val Acc: 78.01%\n",
      "-------------\n",
      "Epoch: 68\n",
      "Train Loss: 0.325, Train Acc: 98.62%\n",
      "Val Loss: 0.524084, Val Acc: 77.97%\n",
      "-------------\n",
      "Epoch: 69\n",
      "Train Loss: 0.325, Train Acc: 98.62%\n",
      "Val Loss: 0.524111, Val Acc: 77.98%\n",
      "-------------\n",
      "Epoch: 70\n",
      "Train Loss: 0.325, Train Acc: 98.62%\n",
      "Val Loss: 0.525829, Val Acc: 77.83%\n",
      "-------------\n",
      "Epoch: 71\n",
      "Train Loss: 0.325, Train Acc: 98.63%\n",
      "Val Loss: 0.525481, Val Acc: 77.81%\n",
      "-------------\n",
      "Epoch: 72\n",
      "Train Loss: 0.325, Train Acc: 98.63%\n",
      "Val Loss: 0.525659, Val Acc: 77.85%\n",
      "-------------\n",
      "Epoch: 73\n",
      "Train Loss: 0.324, Train Acc: 98.64%\n",
      "Val Loss: 0.525401, Val Acc: 77.85%\n",
      "-------------\n",
      "Epoch: 74\n",
      "Train Loss: 0.325, Train Acc: 98.63%\n",
      "Val Loss: 0.525177, Val Acc: 77.97%\n",
      "-------------\n",
      "Epoch: 75\n",
      "Train Loss: 0.325, Train Acc: 98.63%\n",
      "Val Loss: 0.525022, Val Acc: 77.91%\n",
      "-------------\n",
      "Epoch: 76\n",
      "Train Loss: 0.325, Train Acc: 98.63%\n",
      "Val Loss: 0.524918, Val Acc: 77.95%\n",
      "-------------\n",
      "Epoch: 77\n",
      "Train Loss: 0.325, Train Acc: 98.63%\n",
      "Val Loss: 0.524835, Val Acc: 77.97%\n",
      "-------------\n",
      "Epoch: 78\n",
      "Train Loss: 0.325, Train Acc: 98.63%\n",
      "Val Loss: 0.524782, Val Acc: 77.98%\n",
      "-------------\n",
      "Epoch: 79\n",
      "Train Loss: 0.324, Train Acc: 98.64%\n",
      "Val Loss: 0.524745, Val Acc: 77.98%\n",
      "-------------\n",
      "Epoch: 80\n",
      "Train Loss: 0.324, Train Acc: 98.64%\n",
      "Val Loss: 0.524719, Val Acc: 77.99%\n",
      "-------------\n",
      "Epoch: 81\n",
      "Train Loss: 0.325, Train Acc: 98.63%\n",
      "Val Loss: 0.524779, Val Acc: 78.03%\n",
      "-------------\n",
      "Epoch: 82\n",
      "Train Loss: 0.326, Train Acc: 98.53%\n",
      "Val Loss: 0.524703, Val Acc: 77.82%\n",
      "-------------\n",
      "Epoch: 83\n",
      "Train Loss: 0.325, Train Acc: 98.58%\n",
      "Val Loss: 0.527394, Val Acc: 77.63%\n",
      "-------------\n",
      "Epoch: 84\n",
      "Train Loss: 0.325, Train Acc: 98.64%\n",
      "Val Loss: 0.519577, Val Acc: 78.23%\n",
      "-------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85\n",
      "Train Loss: 0.324, Train Acc: 98.66%\n",
      "Val Loss: 0.519634, Val Acc: 78.35%\n",
      "-------------\n",
      "Epoch: 86\n",
      "Train Loss: 0.324, Train Acc: 98.65%\n",
      "Val Loss: 0.520698, Val Acc: 78.28%\n",
      "-------------\n",
      "Epoch: 87\n",
      "Train Loss: 0.324, Train Acc: 98.67%\n",
      "Val Loss: 0.519723, Val Acc: 78.43%\n",
      "-------------\n",
      "Epoch: 88\n",
      "Train Loss: 0.324, Train Acc: 98.67%\n",
      "Val Loss: 0.519515, Val Acc: 78.36%\n",
      "-------------\n",
      "Epoch: 89\n",
      "Train Loss: 0.324, Train Acc: 98.67%\n",
      "Val Loss: 0.519470, Val Acc: 78.39%\n",
      "-------------\n",
      "Epoch: 90\n",
      "Train Loss: 0.324, Train Acc: 98.67%\n",
      "Val Loss: 0.519657, Val Acc: 78.40%\n",
      "-------------\n",
      "Epoch: 91\n",
      "Train Loss: 0.324, Train Acc: 98.69%\n",
      "Val Loss: 0.520045, Val Acc: 78.36%\n",
      "-------------\n",
      "Epoch: 92\n",
      "Train Loss: 0.324, Train Acc: 98.69%\n",
      "Val Loss: 0.519917, Val Acc: 78.35%\n",
      "-------------\n",
      "Epoch: 93\n",
      "Train Loss: 0.324, Train Acc: 98.69%\n",
      "Val Loss: 0.519801, Val Acc: 78.35%\n",
      "-------------\n",
      "Epoch: 94\n",
      "Train Loss: 0.324, Train Acc: 98.69%\n",
      "Val Loss: 0.519718, Val Acc: 78.36%\n",
      "-------------\n",
      "Epoch: 95\n",
      "Train Loss: 0.324, Train Acc: 98.69%\n",
      "Val Loss: 0.519665, Val Acc: 78.38%\n",
      "-------------\n",
      "Epoch: 96\n",
      "Train Loss: 0.324, Train Acc: 98.70%\n",
      "Val Loss: 0.519644, Val Acc: 78.36%\n",
      "-------------\n",
      "Epoch: 97\n",
      "Train Loss: 0.324, Train Acc: 98.70%\n",
      "Val Loss: 0.519637, Val Acc: 78.38%\n",
      "-------------\n",
      "Epoch: 98\n",
      "Train Loss: 0.324, Train Acc: 98.69%\n",
      "Val Loss: 0.519646, Val Acc: 78.39%\n",
      "-------------\n",
      "Epoch: 99\n",
      "Train Loss: 0.324, Train Acc: 98.69%\n",
      "Val Loss: 0.520545, Val Acc: 78.30%\n",
      "-------------\n",
      "Epoch: 100\n",
      "Train Loss: 0.324, Train Acc: 98.70%\n",
      "Val Loss: 0.520384, Val Acc: 78.10%\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "num_bad_epochs = 0\n",
    "epoch = 0\n",
    "least_loss = float('inf')\n",
    "training_stats = pd.DataFrame(columns=['Epoch', 'Train_Loss', 'Train_Acc', 'Val_Loss', 'Val_Acc'])\n",
    "\n",
    "while(True):\n",
    "    train_loss, train_acc = train_model(model, optim, train_iter, epoch, batch_size)\n",
    "    val_loss, val_acc = eval_model(model, valid_iter) \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    if val_loss < least_loss:\n",
    "        least_loss = val_loss\n",
    "        num_bad_epochs = 0\n",
    "        print(\"*** Least validation loss\")\n",
    "        torch.save(model.state_dict(), \"IMDB_Soft_Attn_100\")\n",
    "    else:\n",
    "        num_bad_epochs += 1\n",
    "#     print(f'Epoch: {epoch+1:2}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%') \n",
    "    print(f'Val Loss: {val_loss:3f}, Val Acc: {val_acc:.2f}%')\n",
    "    print(\"-------------\")\n",
    "    \n",
    "    training_stats = training_stats.append(\n",
    "        pd.Series([epoch+1, train_loss, train_acc, val_loss, val_acc], index=training_stats.columns), \n",
    "        ignore_index=True)\n",
    "#     if num_bad_epochs >= 10:\n",
    "#         break\n",
    "        \n",
    "    epoch += 1\n",
    "    if epoch == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "vwMLkn7s7cnC",
    "outputId": "86182669-19b9-4b8d-b9a4-815acf8d240e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.535, Test Acc: 77.25\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = eval_model(loaded_model, test_iter)\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kj2OOxjKaAIm",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "['this', 'is', 'one', 'of', 'the', 'best', 'creation', 'of', 'nolan.', 'i', 'can', 'say,', \"it's\", 'his', 'magnum', 'opus.', 'loved', 'the', 'soundtrack', 'and', 'especially', 'those', 'creative', 'dialogues.']\n",
      "prediction = tensor([[1.0000e+00, 2.0382e-38]], grad_fn=<SqueezeBackward1>)\n",
      "this 0.04583347216248512\n",
      "is 0.04616454243659973\n",
      "one 0.04620068892836571\n",
      "of 0.03251994773745537\n",
      "the 0.04598066955804825\n",
      "best 0.04620719701051712\n",
      "creation 0.023172136396169662\n",
      "of 0.023873215541243553\n",
      "nolan. 0.04620625078678131\n",
      "i 0.04415389150381088\n",
      "can 0.03162219747900963\n",
      "say, 0.01940092258155346\n",
      "it's 0.04559541493654251\n",
      "his 0.04559972509741783\n",
      "magnum 0.044981393963098526\n",
      "opus. 0.046198125928640366\n",
      "loved 0.04620957747101784\n",
      "the 0.04616222903132439\n",
      "soundtrack 0.04620847478508949\n",
      "and 0.046206723898649216\n",
      "especially 0.04578340798616409\n",
      "those 0.046201933175325394\n",
      "creative 0.04331861436367035\n",
      "dialogues. 0.0461992472410202\n",
      "tensor(0)\n",
      "------------\n",
      "['ohh,', 'such', 'a', 'ridiculous', 'movie.', 'not', 'gonna', 'recommend', 'it', 'to', 'anyone.', 'complete', 'waste', 'of', 'time', 'and', 'money.']\n",
      "prediction = tensor([[1.3221e-41, 1.0000e+00]], grad_fn=<SqueezeBackward1>)\n",
      "ohh, 0.05977955088019371\n",
      "such 0.05940583720803261\n",
      "a 0.04903075844049454\n",
      "ridiculous 0.05977954342961311\n",
      "movie. 0.0597786083817482\n",
      "not 0.0547560416162014\n",
      "gonna 0.05967453122138977\n",
      "recommend 0.059779319912195206\n",
      "it 0.05977943539619446\n",
      "to 0.05977952852845192\n",
      "anyone. 0.05977955088019371\n",
      "complete 0.05977955088019371\n",
      "waste 0.05977955088019371\n",
      "of 0.05977955088019371\n",
      "time 0.05977955088019371\n",
      "and 0.05977955088019371\n",
      "money. 0.05977955088019371\n",
      "tensor(1)\n",
      "------------\n",
      "['i', 'admire', 'deepa', 'mehta', 'and', 'this', 'movie', 'is', 'a', 'masterpiece', '.', 'i', '’d', 'recom-', 'mend', 'to', 'buy', 'this', 'movie', 'on', 'dvd', 'because', 'it', '’s', 'a', 'movie', 'you', 'might', 'want', 'to', 'watch', 'more', 'often', 'than', 'just', 'once', '.', 'and', 'trust', 'me', ',', 'you', '’d', 'still', 'find', 'little', 'meaningful', 'details', 'after', 'watching', 'it', 'several', 'times.', 'the', 'characters', '-', 'except', 'for', 'the', 'grandmother', 'perhaps', '-', 'are', 'all', 'very', 'bal-', 'anced', ',', 'no', 'black', 'and', 'white', '.', 'even', 'though', 'you', 'follow', 'the', 'story', 'from', 'the', 'perspective', 'of', 'the', 'two', 'protagonists', ',', 'there', 'is', 'also', 'empathy', 'for', 'the', 'other', 'characters.', 'i', 'think', 'the', 'imdb', 'rating', 'for', 'the', 'movie', 'is', 'far', 'too', 'low', '-', 'probably', 'due', 'to', 'its', 'politically', 'controversial', 'content', '.']\n",
      "prediction = tensor([[1.0000e+00, 2.1768e-15]], grad_fn=<SqueezeBackward1>)\n",
      "i 0.00982747133821249\n",
      "admire 0.010156987234950066\n",
      "deepa 0.010172129608690739\n",
      "mehta 0.010172192938625813\n",
      "and 0.010171971283853054\n",
      "this 0.010162144899368286\n",
      "movie 0.010172707028687\n",
      "is 0.010172604583203793\n",
      "a 0.010167373344302177\n",
      "masterpiece 0.010173858143389225\n",
      ". 0.010173494927585125\n",
      "i 0.010095624253153801\n",
      "’d 0.010134255513548851\n",
      "recom- 0.01017020083963871\n",
      "mend 0.007245535496622324\n",
      "to 0.0041580963879823685\n",
      "buy 0.010151096619665623\n",
      "this 0.009319302625954151\n",
      "movie 0.008719824254512787\n",
      "on 0.007391858380287886\n",
      "dvd 0.010152827017009258\n",
      "because 0.009793899022042751\n",
      "it 0.009932667948305607\n",
      "’s 0.010103194043040276\n",
      "a 0.010117791593074799\n",
      "movie 0.010167259722948074\n",
      "you 0.009898527525365353\n",
      "might 0.004160592798143625\n",
      "want 0.008704946376383305\n",
      "to 0.0039954474195837975\n",
      "watch 0.005413741804659367\n",
      "more 0.009187170304358006\n",
      "often 0.010173826478421688\n",
      "than 0.009921235963702202\n",
      "just 0.009690782986581326\n",
      "once 0.010173310525715351\n",
      ". 0.010169338434934616\n",
      "and 0.010141860693693161\n",
      "trust 0.010173169896006584\n",
      "me 0.010144644416868687\n",
      ", 0.010125810280442238\n",
      "you 0.010088099166750908\n",
      "’d 0.010173594579100609\n",
      "still 0.010173017159104347\n",
      "find 0.010035957209765911\n",
      "little 0.009990120306611061\n",
      "meaningful 0.010161048732697964\n",
      "details 0.010166380554437637\n",
      "after 0.007841157726943493\n",
      "watching 0.009048178791999817\n",
      "it 0.010079577565193176\n",
      "several 0.0071135773323476315\n",
      "times. 0.00582509022206068\n",
      "the 0.0078069837763905525\n",
      "characters 0.010171787813305855\n",
      "- 0.005525725428014994\n",
      "except 0.005948977544903755\n",
      "for 0.004671582952141762\n",
      "the 0.008263831026852131\n",
      "grandmother 0.004423975944519043\n",
      "perhaps 0.009987051598727703\n",
      "- 0.007419475354254246\n",
      "are 0.005523532163351774\n",
      "all 0.004133467562496662\n",
      "very 0.003964585717767477\n",
      "bal- 0.005159782711416483\n",
      "anced 0.006861993111670017\n",
      ", 0.004161641467362642\n",
      "no 0.010173842310905457\n",
      "black 0.007118732202798128\n",
      "and 0.00592083390802145\n",
      "white 0.006475875619798899\n",
      ". 0.005181352142244577\n",
      "even 0.006252172403037548\n",
      "though 0.01001243107020855\n",
      "you 0.008908919990062714\n",
      "follow 0.009521053172647953\n",
      "the 0.010158377699553967\n",
      "story 0.010170970112085342\n",
      "from 0.006285006646066904\n",
      "the 0.010136782191693783\n",
      "perspective 0.010166262276470661\n",
      "of 0.00810256414115429\n",
      "the 0.01015736348927021\n",
      "two 0.009802551940083504\n",
      "protagonists 0.010132997296750546\n",
      ", 0.004847413394600153\n",
      "there 0.004635245073586702\n",
      "is 0.008513934910297394\n",
      "also 0.0054671140387654305\n",
      "empathy 0.0049570477567613125\n",
      "for 0.004802599083632231\n",
      "the 0.009727942757308483\n",
      "other 0.005682411137968302\n",
      "characters. 0.010171003639698029\n",
      "i 0.008691348135471344\n",
      "think 0.006979265715926886\n",
      "the 0.0101718008518219\n",
      "imdb 0.010174263268709183\n",
      "rating 0.010174263268709183\n",
      "for 0.010174261406064034\n",
      "the 0.010174261406064034\n",
      "movie 0.010174263268709183\n",
      "is 0.010174263268709183\n",
      "far 0.010174263268709183\n",
      "too 0.010174263268709183\n",
      "low 0.010151703841984272\n",
      "- 0.009955378249287605\n",
      "probably 0.010173262096941471\n",
      "due 0.009949465282261372\n",
      "to 0.006832578219473362\n",
      "its 0.010174187831580639\n",
      "politically 0.010174263268709183\n",
      "controversial 0.010021389462053776\n",
      "content 0.009778921492397785\n",
      ". 0.0060985637828707695\n",
      "tensor(0)\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "def test_sentence(test_sen):\n",
    "    test_sen_list = TEXT.preprocess(test_sen)\n",
    "    print(test_sen_list)\n",
    "    test_sen = [[TEXT.vocab.stoi[x] for x in test_sen_list]]\n",
    "    # print(test_sen)\n",
    "\n",
    "    test_sen = np.asarray(test_sen)\n",
    "    test_sen = torch.LongTensor(test_sen)\n",
    "    test_tensor = Variable(test_sen, volatile=True)\n",
    "\n",
    "    # print(test_tensor)\n",
    "    loaded_model.eval()\n",
    "    prediction, alpha = loaded_model(test_tensor, is_train = False)\n",
    "    print(\"prediction =\", prediction)\n",
    "#     print(\"g =\", g_t)\n",
    "    \n",
    "    for i in range(len(test_sen_list)):\n",
    "        print(test_sen_list[i], float(alpha[0][i][0]))\n",
    "    out_class = torch.argmax(prediction)\n",
    "    return out_class\n",
    "\n",
    "\n",
    "''' Let us now predict the sentiment on a single sentence just for the testing purpose. '''\n",
    "test_sen1 = \"This is one of the best creation of Nolan. I can say, it's his magnum opus. Loved the soundtrack and especially those creative dialogues.\"\n",
    "test_sen2 = \"Ohh, such a ridiculous movie. Not gonna recommend it to anyone. Complete waste of time and money.\"\n",
    "test_sen3 = \"I admire Deepa Mehta and this movie is a masterpiece . I ’d recom- mend to buy this movie on DVD because it ’s a movie you might want to watch more often than just once . And trust me , you ’d still find little meaningful details after watching it several times. The characters - except for the grandmother perhaps - are all very bal- anced , no black and white . Even though you follow the story from the perspective of the two protagonists , there is also empathy for the other characters. I think the IMDb rating for the movie is far too low - probably due to its politically controversial content .\"\n",
    "\n",
    "print('------------')\n",
    "x = test_sentence(test_sen1)\n",
    "print(x)\n",
    "print('------------')\n",
    "x = test_sentence(test_sen2)\n",
    "print(x)\n",
    "print('------------')\n",
    "x = test_sentence(test_sen3)\n",
    "print(x)\n",
    "print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'one', 'of', 'the', 'best', 'creation', 'of', 'nolan.', 'i', 'can', 'say,', \"it's\", 'his', 'magnum', 'opus.', 'loved', 'the', 'soundtrack', 'and', 'especially', 'those', 'creative', 'dialogues.']\n",
      "prediction = tensor([[1.0000e+00, 2.0382e-38]], grad_fn=<SqueezeBackward1>)\n",
      "this 0.04583347216248512\n",
      "is 0.04616454243659973\n",
      "one 0.04620068892836571\n",
      "of 0.03251994773745537\n",
      "the 0.04598066955804825\n",
      "best 0.04620719701051712\n",
      "creation 0.023172136396169662\n",
      "of 0.023873215541243553\n",
      "nolan. 0.04620625078678131\n",
      "i 0.04415389150381088\n",
      "can 0.03162219747900963\n",
      "say, 0.01940092258155346\n",
      "it's 0.04559541493654251\n",
      "his 0.04559972509741783\n",
      "magnum 0.044981393963098526\n",
      "opus. 0.046198125928640366\n",
      "loved 0.04620957747101784\n",
      "the 0.04616222903132439\n",
      "soundtrack 0.04620847478508949\n",
      "and 0.046206723898649216\n",
      "especially 0.04578340798616409\n",
      "those 0.046201933175325394\n",
      "creative 0.04331861436367035\n",
      "dialogues. 0.0461992472410202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "test_sen = \"This is one of the best creation of Nolan. I can say, it's his magnum opus. Loved the soundtrack and especially those creative dialogues.\"\n",
    "test_sen_list = TEXT.preprocess(test_sen)\n",
    "print(test_sen_list)\n",
    "test_sen = [[TEXT.vocab.stoi[x] for x in test_sen_list]]\n",
    "# print(test_sen)\n",
    "\n",
    "test_sen = np.asarray(test_sen)\n",
    "test_sen = torch.LongTensor(test_sen)\n",
    "test_tensor = Variable(test_sen, volatile=True)\n",
    "\n",
    "# print(test_tensor)\n",
    "loaded_model.eval()\n",
    "prediction, alpha = loaded_model(test_tensor, is_train = False)\n",
    "print(\"prediction =\", prediction)\n",
    "#     print(\"g =\", g_t)\n",
    "\n",
    "for i in range(len(test_sen_list)):\n",
    "    print(test_sen_list[i], float(alpha[0][i][0]))\n",
    "out_class = torch.argmax(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Train_Loss</th>\n",
       "      <th>Train_Acc</th>\n",
       "      <th>Val_Loss</th>\n",
       "      <th>Val_Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.603291</td>\n",
       "      <td>67.567413</td>\n",
       "      <td>0.543198</td>\n",
       "      <td>74.747340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.476999</td>\n",
       "      <td>82.638254</td>\n",
       "      <td>0.532478</td>\n",
       "      <td>76.196809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.413601</td>\n",
       "      <td>89.510969</td>\n",
       "      <td>0.547901</td>\n",
       "      <td>75.119681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.381792</td>\n",
       "      <td>92.824497</td>\n",
       "      <td>0.521762</td>\n",
       "      <td>77.845745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.364260</td>\n",
       "      <td>94.572669</td>\n",
       "      <td>0.532190</td>\n",
       "      <td>76.715426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96.0</td>\n",
       "      <td>0.323829</td>\n",
       "      <td>98.703154</td>\n",
       "      <td>0.519644</td>\n",
       "      <td>78.364362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97.0</td>\n",
       "      <td>0.323886</td>\n",
       "      <td>98.697441</td>\n",
       "      <td>0.519637</td>\n",
       "      <td>78.377660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98.0</td>\n",
       "      <td>0.323944</td>\n",
       "      <td>98.691728</td>\n",
       "      <td>0.519646</td>\n",
       "      <td>78.390957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99.0</td>\n",
       "      <td>0.323944</td>\n",
       "      <td>98.691728</td>\n",
       "      <td>0.520545</td>\n",
       "      <td>78.297872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.323886</td>\n",
       "      <td>98.697441</td>\n",
       "      <td>0.520384</td>\n",
       "      <td>78.098404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Epoch  Train_Loss  Train_Acc  Val_Loss    Val_Acc\n",
       "0     1.0    0.603291  67.567413  0.543198  74.747340\n",
       "1     2.0    0.476999  82.638254  0.532478  76.196809\n",
       "2     3.0    0.413601  89.510969  0.547901  75.119681\n",
       "3     4.0    0.381792  92.824497  0.521762  77.845745\n",
       "4     5.0    0.364260  94.572669  0.532190  76.715426\n",
       "..    ...         ...        ...       ...        ...\n",
       "95   96.0    0.323829  98.703154  0.519644  78.364362\n",
       "96   97.0    0.323886  98.697441  0.519637  78.377660\n",
       "97   98.0    0.323944  98.691728  0.519646  78.390957\n",
       "98   99.0    0.323944  98.691728  0.520545  78.297872\n",
       "99  100.0    0.323886  98.697441  0.520384  78.098404\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(training_stats['Train_Loss'], label=\"Train\")\n",
    "plt.plot(training_stats['Val_Loss'], label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_stats['Train_Acc'], label=\"Train\")\n",
    "plt.plot(training_stats['Val_Acc'], label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GANet_Torch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
